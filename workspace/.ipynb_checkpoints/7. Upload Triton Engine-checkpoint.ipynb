{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae552523-6293-460d-9c37-fbdf56e532f2",
   "metadata": {},
   "source": [
    "# Convert Model\n",
    "\n",
    "There are 2 ways to convert yolo model to tensorrt engine. <br>\n",
    "> 1. Use Ultralytics api\n",
    "> 2. Use tensorRT API or trtexec command\n",
    "\n",
    "During the validation, #1 method didn't work with Triton inference server.<br>\n",
    "~~So We will use the tensorRT engine which is converted by trtexec command.~~ <br>\n",
    "But With some work-arounds, Ultralytics API works to convert model from pt to tensorrt engine. <br> \n",
    "In this tutorial, we will walk around end to end example for `Serving Yolo model as tensorrt Engine with Triton Inference Server.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8efa5-338b-4327-b9be-f96de1a38d80",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Install python packages in requirements.txt\n",
    "* ~~Create Persistent Volume (RWX permission) via Kubeflow to save the model between pods and Create Notebook with it. ( PV is mounted to **/mnt/yolo-model** in this example)~~\n",
    "* Need GPU to convert TensorRT engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37662b00-ceaf-4080-8b58-18b288196427",
   "metadata": {},
   "source": [
    "### check the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335c8682-fca0-4f51-8f57-4df063dca76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 8400])\n",
      "List output: [torch.Size([1, 65, 80, 80]), torch.Size([1, 65, 40, 40]), torch.Size([1, 65, 20, 20])]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model_path = \"/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/\"\n",
    "model_name = \"best.pt\"\n",
    "\n",
    "torch_model = YOLO(model_path + model_name).model\n",
    "torch_model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 640, 640)\n",
    "with torch.no_grad():\n",
    "    outputs = torch_model(dummy_input)\n",
    "\n",
    "for out in outputs:\n",
    "    # Some outputs are lists; checking each element carefully\n",
    "    if isinstance(out, torch.Tensor):\n",
    "        print(out.shape)\n",
    "    else:\n",
    "        print(\"List output:\", [o.shape for o in out if hasattr(o, 'shape')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f56d2d-ed98-4f12-9797-59fb3bff7e55",
   "metadata": {},
   "source": [
    "## 1. Convert yolo model to TensorRT Engine\n",
    "#### Before Directly Convert model with YOLO API, There are 2 points to be handled\n",
    "* Block some lines in ultralytics/engine/exporter.py to prevent well know issue with Triton Inference Server ( https://github.com/ultralytics/ultralytics/issues/4597#issuecomment-1694948850 )\n",
    "* Make sure below snippet is blocked in your environment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c27d92eb-68a4-4caa-b77b-a627c4a62833",
   "metadata": {},
   "source": [
    "(base) geun-tak.roh-hpe.com@trt-test-0:/opt/conda/lib/python3.11/site-packages/ultralytics/utils$ diff export.py export.py_org \n",
    "227,231c227,231\n",
    "< #        # Metadata\n",
    "< #        if metadata is not None:\n",
    "< #            meta = json.dumps(metadata)\n",
    "< #            t.write(len(meta).to_bytes(4, byteorder=\"little\", signed=True))\n",
    "< #            t.write(meta.encode())\n",
    "---\n",
    ">         # Metadata\n",
    ">         if metadata is not None:\n",
    ">             meta = json.dumps(metadata)\n",
    ">             t.write(len(meta).to_bytes(4, byteorder=\"little\", signed=True))\n",
    ">             t.write(meta.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "313deed6-beb4-41a6-8257-f44e4a3fe3be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.144 üöÄ Python-3.11.9 torch-2.7.0+cu126 CUDA:0 (NVIDIA L40S, 45589MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,187 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 3.3s, saved as '/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.onnx' (36.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.11.0.33...\n",
      "[05/28/2025-06:00:22] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -1567, GPU +0, now: CPU 5898, GPU 1460 (MiB)\n",
      "[05/28/2025-06:00:22] [TRT] [I] ----------------------------------------------------------------\n",
      "[05/28/2025-06:00:22] [TRT] [I] Input filename:   /mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.onnx\n",
      "[05/28/2025-06:00:22] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[05/28/2025-06:00:22] [TRT] [I] Opset version:    19\n",
      "[05/28/2025-06:00:22] [TRT] [I] Producer name:    pytorch\n",
      "[05/28/2025-06:00:22] [TRT] [I] Producer version: 2.7.0\n",
      "[05/28/2025-06:00:22] [TRT] [I] Domain:           \n",
      "[05/28/2025-06:00:22] [TRT] [I] Model version:    0\n",
      "[05/28/2025-06:00:22] [TRT] [I] Doc string:       \n",
      "[05/28/2025-06:00:22] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 5, -1) DataType.FLOAT\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mTensorRT:\u001b[0m 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as /mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine\n",
      "[05/28/2025-06:00:22] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[05/28/2025-06:01:34] [TRT] [I] Compiler backend is used during engine build.\n",
      "[05/28/2025-06:03:05] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[05/28/2025-06:03:06] [TRT] [I] Total Host Persistent Memory: 536544 bytes\n",
      "[05/28/2025-06:03:06] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[05/28/2025-06:03:06] [TRT] [I] Max Scratch Memory: 42611200 bytes\n",
      "[05/28/2025-06:03:06] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 166 steps to complete.\n",
      "[05/28/2025-06:03:06] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 7.80944ms to assign 11 blocks to 166 nodes requiring 92583424 bytes.\n",
      "[05/28/2025-06:03:06] [TRT] [I] Total Activation Memory: 92582400 bytes\n",
      "[05/28/2025-06:03:06] [TRT] [I] Total Weights Memory: 18857352 bytes\n",
      "[05/28/2025-06:03:06] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[05/28/2025-06:03:06] [TRT] [I] Engine generation completed in 163.975 seconds.\n",
      "[05/28/2025-06:03:06] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 530 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 169.8s, saved as '/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine' (21.5 MB)\n",
      "\n",
      "Export complete (169.9s)\n",
      "Results saved to \u001b[1m/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine imgsz=640 data=./datasets/data.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model_path = \"/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/\"\n",
    "model_name = \"best.pt\"\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(model_path + model_name)  # load an official model\n",
    "\n",
    "# Retrieve metadata during export. Metadata needs to be added to config.pbtxt. See next section.\n",
    "metadata = []\n",
    "\n",
    "def export_cb(exporter):\n",
    "    metadata.append(exporter.metadata)\n",
    "\n",
    "model.add_callback(\"on_export_end\", export_cb)\n",
    "\n",
    "# Export the model\n",
    "engine_file = model.export(format=\"engine\", dynamic=True,half=True,device=0)\n",
    "\n",
    "data = \"\"\"\n",
    "# Add metadata\n",
    "parameters {\n",
    "  key: \"metadata\"\n",
    "  value {\n",
    "    string_value: \"%s\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "name: \"license_detector\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"images\"\n",
    "    dims: [ -1, 3, 640, 640 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output0\"\n",
    "    dims: [ -1, 5, 8400 ]\n",
    "  }\n",
    "]\n",
    "\"\"\" % metadata[0]  # noqa\n",
    "\n",
    "with open(\"config.pbtxt\", \"w\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2feb88b2-21d8-4d71-a55a-0c9c73c4b92f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.144 üöÄ Python-3.11.9 torch-2.7.0+cu126 CUDA:0 (NVIDIA L40S, 45589MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,443,760 parameters, 0 gradients, 21.5 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11s.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (18.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.53...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 3.4s, saved as 'yolo11s.onnx' (36.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.11.0.33...\n",
      "[05/28/2025-06:38:25] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU -1567, GPU +0, now: CPU 5887, GPU 1566 (MiB)\n",
      "[05/28/2025-06:38:25] [TRT] [I] ----------------------------------------------------------------\n",
      "[05/28/2025-06:38:25] [TRT] [I] Input filename:   yolo11s.onnx\n",
      "[05/28/2025-06:38:25] [TRT] [I] ONNX IR version:  0.0.9\n",
      "[05/28/2025-06:38:25] [TRT] [I] Opset version:    19\n",
      "[05/28/2025-06:38:25] [TRT] [I] Producer name:    pytorch\n",
      "[05/28/2025-06:38:25] [TRT] [I] Producer version: 2.7.0\n",
      "[05/28/2025-06:38:25] [TRT] [I] Domain:           \n",
      "[05/28/2025-06:38:25] [TRT] [I] Model version:    0\n",
      "[05/28/2025-06:38:25] [TRT] [I] Doc string:       \n",
      "[05/28/2025-06:38:25] [TRT] [I] ----------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(-1, 3, -1, -1) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(-1, 84, -1) DataType.FLOAT\n",
      "WARNING ‚ö†Ô∏è \u001b[34m\u001b[1mTensorRT:\u001b[0m 'dynamic=True' model requires max batch size, i.e. 'batch=16'\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo11s.engine\n",
      "[05/28/2025-06:38:25] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[05/28/2025-06:39:35] [TRT] [I] Compiler backend is used during engine build.\n",
      "[05/28/2025-06:41:06] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[05/28/2025-06:41:07] [TRT] [I] Total Host Persistent Memory: 536544 bytes\n",
      "[05/28/2025-06:41:07] [TRT] [I] Total Device Persistent Memory: 0 bytes\n",
      "[05/28/2025-06:41:07] [TRT] [I] Max Scratch Memory: 42611200 bytes\n",
      "[05/28/2025-06:41:07] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 181 steps to complete.\n",
      "[05/28/2025-06:41:07] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 9.36297ms to assign 11 blocks to 181 nodes requiring 93197824 bytes.\n",
      "[05/28/2025-06:41:07] [TRT] [I] Total Activation Memory: 93196800 bytes\n",
      "[05/28/2025-06:41:07] [TRT] [I] Total Weights Memory: 18912648 bytes\n",
      "[05/28/2025-06:41:07] [TRT] [I] Compiler backend is used during engine execution.\n",
      "[05/28/2025-06:41:07] [TRT] [I] Engine generation completed in 162.497 seconds.\n",
      "[05/28/2025-06:41:07] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 1041 MiB\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 168.5s, saved as 'yolo11s.engine' (21.9 MB)\n",
      "\n",
      "Export complete (168.6s)\n",
      "Results saved to \u001b[1m/mnt/user/LPD/finetune\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11s.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=yolo11s.engine imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "# Load a model\n",
    "vehicle = YOLO('yolo11s')  # load an official model\n",
    "\n",
    "# Retrieve metadata during export. Metadata needs to be added to config.pbtxt. See next section.\n",
    "metadata = []\n",
    "\n",
    "def export_cb(exporter):\n",
    "    metadata.append(exporter.metadata)\n",
    "\n",
    "vehicle.add_callback(\"on_export_end\", export_cb)\n",
    "\n",
    "# Export the model\n",
    "engine_file = vehicle.export(format=\"engine\", dynamic=True,half=True,device=0)\n",
    "\n",
    "data = \"\"\"\n",
    "# Add metadata\n",
    "parameters {\n",
    "  key: \"metadata\"\n",
    "  value {\n",
    "    string_value: \"%s\"\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "name: \"vehicle_detector\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size : 0\n",
    "input [\n",
    "  {\n",
    "    name: \"images\"\n",
    "    dims: [ -1, 3, 640, 640 ]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output0\"\n",
    "    dims: [ -1, 84, 8400 ]\n",
    "  }\n",
    "]\n",
    "\"\"\" % metadata[0]  # noqa\n",
    "\n",
    "with open(\"config.pbtxt\", \"w\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7b562-f12f-46f0-be3c-2afd41d41b27",
   "metadata": {},
   "source": [
    "## 2. Create Triton compatible Directory\n",
    "**Triton model repository layout**<br> Refer : https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html#repository-layout \n",
    "```\n",
    "  <model-repository-path>/\n",
    "    <model-name>/\n",
    "      [config.pbtxt]\n",
    "      [<output-labels-file> ...]\n",
    "      [configs]/\n",
    "        [<custom-config-file> ...]\n",
    "      <version>/\n",
    "        <model-definition-file>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b41b582-2acc-4929-b3f3-5da79e64cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat '/mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p triton_engines/license_detector/1\n",
    "!mv config.pbtxt triton_engines/license_detector\n",
    "!mv /mnt/user/LPD/finetune/artifact_downloads/14be0ff37e03403b9f50b3cd4e8dd7a5/weights/best.engine triton_engines/license_detector/1/model.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a7aa0c-2f2d-4fc1-8cb9-801254c94fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p triton_engines/vehicle_detector/1\n",
    "!mv config.pbtxt triton_engines/vehicle_detector\n",
    "!mv yolo11s.engine triton_engines/vehicle_detector/1/model.plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e006dde-a214-4095-938e-349aad678ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -rlt triton_models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3c3da-3b8e-43ea-8387-7ccba7bd3a12",
   "metadata": {},
   "source": [
    "# Upload engine into the MLflow\n",
    "\n",
    "In this part, we will use example files in triton inference server github repo. <br>\n",
    "**Ref** : https://github.com/triton-inference-server/server/tree/main/deploy/mlflow-triton-plugin#triton-flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f184d882-6469-4a3a-872e-dcc34e83f417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "export MLFLOW_TRACKING_URI='https://mlflow.ingress.pcai0103.sy6.hpecolo.net'\n",
      "export MLFLOW_TRACKING_TOKEN=$(cat /etc/secrets/ezua/.auth_token)\n",
      "export MLFLOW_S3_ENDPOINT_URL='http://local-s3-service.ezdata-system.svc.cluster.local:30000'\n",
      "python mlflow_scripts/publish_model_to_mlflow.py --model_name license-detector --model_directory ./triton_engines --flavor triton\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat mlflow_scripts/upload.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5013e85e-9185-420b-be37-8bad788def7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "<module 'triton_flavor' from '/mnt/user/LPD/finetune/mlflow_scripts/triton_flavor.py'>\n",
      "2025/05/28 14:40:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "Registered model 'license-detector' already exists. Creating a new version of this model...\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "2025/05/28 14:40:16 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: license-detector, version 2\n",
      "Created version '2' of model 'license-detector'.\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "s3://mlflow.sy6s171r10/16/b73cfe58b2a54d7a8dd96b157957d06d/artifacts\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "2025/05/28 14:40:16 INFO mlflow.tracking._tracking_service.client: üèÉ View run receptive-asp-793 at: https://mlflow.ingress.pcai0103.sy6.hpecolo.net/#/experiments/16/runs/b73cfe58b2a54d7a8dd96b157957d06d.\n",
      "2025/05/28 14:40:16 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://mlflow.ingress.pcai0103.sy6.hpecolo.net/#/experiments/16.\n",
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 'mlflow.ingress.pcai0103.sy6.hpecolo.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!sh mlflow_scripts/upload.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649f54d-91cf-4e3b-9c7a-fd1d0bfee34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
